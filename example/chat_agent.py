"""Chat Agent with Memory - NeuroMemory ç¤ºä¾‹

ä¸€ä¸ªå¸¦è®°å¿†çš„ç»ˆç«¯å¯¹è¯ Agentï¼Œå±•ç¤º NeuroMemory çš„æ ¸å¿ƒåŠŸèƒ½ï¼š
- æ··åˆè®°å¿†æ¶æ„ï¼šå‘é‡æœç´¢ + å›¾å®ä½“å…³ç³» + KV åå¥½
- å¯¹è¯å†å²ç®¡ç†
- å¯é…ç½®çš„è‡ªåŠ¨è®°å¿†æå–ç­–ç•¥

ç”¨æ³•:
    export DEEPSEEK_API_KEY=your_key
    python example/chat_agent.py

é»˜è®¤ä½¿ç”¨å†…ç½®çš„ HashEmbeddingï¼ˆåŸºäºå“ˆå¸Œï¼Œæ— éœ€é¢å¤–ä¾èµ–ï¼Œé€‚åˆåŠŸèƒ½éªŒè¯ï¼‰ã€‚
å¦‚éœ€çœŸæ­£çš„è¯­ä¹‰æ£€ç´¢ï¼Œå®‰è£… sentence-transformers åä¼šè‡ªåŠ¨å¯ç”¨æœ¬åœ°æ¨¡å‹ã€‚
"""

from __future__ import annotations

import asyncio
import hashlib
import logging
import math
import os
import uuid

from neuromemory import ExtractionStrategy, NeuroMemory, OpenAILLM
from neuromemory.providers.embedding import EmbeddingProvider

USER_ID = "demo-user"
SESSION_ID = str(uuid.uuid4())

# --- Embedding Providers ---


class HashEmbedding(EmbeddingProvider):
    """åŸºäºå“ˆå¸Œçš„ embedding providerï¼Œæ— éœ€é¢å¤–ä¾èµ–ã€‚

    ä½¿ç”¨ SHA-256 å“ˆå¸Œç”Ÿæˆç¡®å®šæ€§å‘é‡ï¼Œä¸æä¾›çœŸæ­£çš„è¯­ä¹‰ç›¸ä¼¼åº¦ï¼Œ
    ä½†å¯ä»¥å®Œæ•´éªŒè¯ NeuroMemory çš„æ‰€æœ‰åŠŸèƒ½æµç¨‹ã€‚
    """

    def __init__(self, dims: int = 1024):
        self._dims = dims

    @property
    def dims(self) -> int:
        return self._dims

    async def embed(self, text: str) -> list[float]:
        digest = hashlib.sha256(text.encode()).digest()
        raw = []
        # ç”¨ SHA-256 çš„ 32 å­—èŠ‚å¾ªç¯å¡«å……åˆ°ç›®æ ‡ç»´åº¦
        for i in range(self._dims):
            byte_val = digest[i % len(digest)] ^ (i // len(digest) & 0xFF)
            raw.append(byte_val / 255.0 * 2 - 1)
        # L2 å½’ä¸€åŒ–
        norm = math.sqrt(sum(x * x for x in raw)) or 1.0
        return [x / norm for x in raw]

    async def embed_batch(self, texts: list[str]) -> list[list[float]]:
        return [await self.embed(t) for t in texts]


MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"


class LocalEmbedding(EmbeddingProvider):
    """ä½¿ç”¨ sentence-transformers çš„æœ¬åœ° embedding providerã€‚

    é¦–æ¬¡ä½¿ç”¨æ—¶è‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼ˆçº¦ 450MBï¼‰ï¼Œä¹‹åä»æœ¬åœ°ç¼“å­˜åŠ è½½ã€‚
    æ¨¡å‹: paraphrase-multilingual-MiniLM-L12-v2 (384 dims, æ”¯æŒä¸­è‹±æ–‡)
    """

    def __init__(self, model_name: str = MODEL_NAME):
        from sentence_transformers import SentenceTransformer

        print(f"åŠ è½½ embedding æ¨¡å‹: {model_name}")
        print("ï¼ˆé¦–æ¬¡è¿è¡Œéœ€è¦ä¸‹è½½ï¼Œä¹‹åä»ç¼“å­˜åŠ è½½ï¼‰")
        self._model = SentenceTransformer(model_name)
        self._dims = self._model.get_sentence_embedding_dimension()
        print(f"æ¨¡å‹åŠ è½½å®Œæˆ (dims={self._dims})")

    @property
    def dims(self) -> int:
        return self._dims

    async def embed(self, text: str) -> list[float]:
        result = await self.embed_batch([text])
        return result[0]

    async def embed_batch(self, texts: list[str]) -> list[list[float]]:
        embeddings = self._model.encode(texts, normalize_embeddings=True)
        return [e.tolist() for e in embeddings]


def create_embedding_provider() -> EmbeddingProvider:
    """è‡ªåŠ¨é€‰æ‹©æœ€ä½³ embedding providerã€‚

    ä¼˜å…ˆä½¿ç”¨ sentence-transformers æœ¬åœ°æ¨¡å‹ï¼ˆçœŸæ­£çš„è¯­ä¹‰æ£€ç´¢ï¼‰ï¼Œ
    å¦‚æœæœªå®‰è£…åˆ™å›é€€åˆ° HashEmbeddingï¼ˆåŠŸèƒ½éªŒè¯æ¨¡å¼ï¼‰ã€‚
    """
    try:
        import sentence_transformers  # noqa: F401
        print("æ£€æµ‹åˆ° sentence-transformersï¼Œä½¿ç”¨æœ¬åœ°è¯­ä¹‰æ¨¡å‹")
        return LocalEmbedding()
    except ImportError:
        print("ä½¿ç”¨å†…ç½® HashEmbeddingï¼ˆåŠŸèƒ½éªŒè¯æ¨¡å¼ï¼Œæ— è¯­ä¹‰ç›¸ä¼¼åº¦ï¼‰")
        print("æç¤º: pip install sentence-transformers å¯å¯ç”¨çœŸæ­£çš„è¯­ä¹‰æ£€ç´¢\n")
        return HashEmbedding(dims=1024)


# --- Provider åˆå§‹åŒ– ---


def create_llm_provider():
    api_key = os.getenv("DEEPSEEK_API_KEY")
    if not api_key:
        raise ValueError("è¯·è®¾ç½® DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡")

    return OpenAILLM(
        api_key=api_key,
        model=os.getenv("LLM_MODEL", "deepseek-chat"),
        base_url=os.getenv("LLM_BASE_URL", "https://api.deepseek.com/v1"),
    )


# --- LLM è°ƒç”¨ ---


async def generate_reply(llm: OpenAILLM, user_input: str, memories: list[dict], history: list[dict]) -> str:
    """è°ƒç”¨ LLM ç”Ÿæˆå›å¤ï¼Œå°†ç›¸å…³è®°å¿†æ³¨å…¥ system promptã€‚"""
    system_parts = ["ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„ AI åŠ©æ‰‹ï¼Œèƒ½å¤Ÿè®°ä½ç”¨æˆ·ä¹‹å‰å‘Šè¯‰ä½ çš„ä¿¡æ¯ã€‚"]

    if memories:
        lines = []
        for m in memories:
            meta = m.get("metadata") or {}
            emotion_hint = ""
            if "emotion" in meta and meta["emotion"]:
                label = meta["emotion"].get("label", "")
                valence = meta["emotion"].get("valence", 0)
                if label:
                    emotion_hint = f" [ç”¨æˆ·å½“æ—¶æ„Ÿåˆ°{label}]"
                elif valence < -0.3:
                    emotion_hint = " [è´Ÿé¢æƒ…ç»ª]"
                elif valence > 0.3:
                    emotion_hint = " [æ­£é¢æƒ…ç»ª]"

            if m.get("source") == "graph":
                lines.append(f"- [å…³ç³»] {m.get('content', '')}")
            elif m.get("memory_type") == "reflection":
                lines.append(f"- [ç†è§£] {m['content']}")
            else:
                score = m.get('score', 0)
                lines.append(f"- {m['content']} (å¾—åˆ†: {score:.2f}){emotion_hint}")
        memory_text = "\n".join(lines)
        system_parts.append(f"\nä½ è®°å¾—å…³äºç”¨æˆ·çš„ä»¥ä¸‹ä¿¡æ¯ï¼š\n{memory_text}")

    system_parts.append("\nè¯·åŸºäºä½ å¯¹ç”¨æˆ·çš„äº†è§£æ¥å›ç­”é—®é¢˜ã€‚å¦‚æœç”¨æˆ·å‘Šè¯‰ä½ æ–°çš„ä¸ªäººä¿¡æ¯ï¼Œè‡ªç„¶åœ°è®°ä½å®ƒã€‚")

    messages = [{"role": "system", "content": "\n".join(system_parts)}]

    # æ·»åŠ æœ€è¿‘çš„å¯¹è¯å†å²ï¼ˆæœ€å¤š 10 è½®ï¼‰
    for msg in history[-20:]:
        messages.append({"role": msg["role"], "content": msg["content"]})

    messages.append({"role": "user", "content": user_input})

    return await llm.chat(messages, temperature=0.7, max_tokens=1024)


# --- å‘½ä»¤å¤„ç† ---


async def handle_memories_command(nm: NeuroMemory, query: str):
    """æœç´¢è®°å¿†ï¼ˆæ··åˆæ£€ç´¢ï¼Œä¸‰å› å­è¯„åˆ†ï¼‰ã€‚"""
    if not query:
        print("  ç”¨æ³•: /memories <æœç´¢è¯>")
        return
    recall_result = await nm.recall(user_id=USER_ID, query=query, limit=5)
    merged = recall_result["merged"]
    if not merged:
        print("  æ²¡æœ‰æ‰¾åˆ°ç›¸å…³è®°å¿†ã€‚")
        return
    print(f"  æ‰¾åˆ° {len(merged)} æ¡ç›¸å…³è®°å¿†ï¼ˆä¸‰å› å­è¯„åˆ†ï¼šç›¸å…³æ€§ Ã— æ—¶æ•ˆæ€§ Ã— é‡è¦æ€§ï¼‰ï¼š")
    for r in merged:
        source = r.get("source", "?")
        meta = r.get("metadata") or {}
        emotion_str = ""
        if "emotion" in meta and meta["emotion"]:
            label = meta["emotion"].get("label", "")
            if label:
                emotion_str = f" ğŸ’­{label}"

        if source == "vector":
            score = r.get("score", 0)
            importance = meta.get("importance", "?")
            print(f"    - [å‘é‡/{r.get('memory_type', '?')}] {r['content']} "
                  f"(å¾—åˆ†: {score:.2f}, é‡è¦æ€§: {importance}){emotion_str}")
        else:
            print(f"    - [å›¾/{r.get('relation', '?')}] {r.get('subject', '?')} â†’ "
                  f"{r.get('object', '?')}: {r.get('content', '')}")


async def handle_reflect_command(nm: NeuroMemory):
    """è§¦å‘åæ€ï¼Œç”Ÿæˆé«˜å±‚æ¬¡æ´å¯Ÿã€‚"""
    print("  æ­£åœ¨åæ€æœ€è¿‘çš„è®°å¿†...")
    try:
        result = await nm.reflect(user_id=USER_ID, limit=50)
        count = result.get("insights_generated", 0)
        facts = result.get("facts_added", 0)
        prefs = result.get("preferences_updated", 0)
        print(f"  åæ€å®Œæˆï¼šæå– {facts} æ¡äº‹å®ï¼Œ{prefs} æ¡åå¥½ï¼Œç”Ÿæˆ {count} æ¡æ´å¯Ÿ")
        if count > 0:
            for ins in result.get("insights", []):
                print(f"    - {ins.get('content', '')}")
    except RuntimeError as e:
        print(f"  åæ€å¤±è´¥: {e}")


async def handle_history_command(nm: NeuroMemory):
    """æŸ¥çœ‹å½“å‰ä¼šè¯å¯¹è¯å†å²ã€‚"""
    messages = await nm.conversations.get_session_messages(
        user_id=USER_ID, session_id=SESSION_ID, limit=50,
    )
    if not messages:
        print("  å½“å‰ä¼šè¯æ²¡æœ‰å¯¹è¯å†å²ã€‚")
        return
    print(f"  å½“å‰ä¼šè¯å…± {len(messages)} æ¡æ¶ˆæ¯ï¼š")
    for msg in messages:
        role = "ğŸ§‘ ç”¨æˆ·" if msg.role == "user" else "ğŸ¤– åŠ©æ‰‹"
        content = msg.content[:80] + "..." if len(msg.content) > 80 else msg.content
        print(f"    {role}: {content}")


async def handle_prefs_command(nm: NeuroMemory, args: str):
    """æŸ¥çœ‹æˆ–è®¾ç½®åå¥½ã€‚"""
    parts = args.strip().split(maxsplit=1)
    if not parts:
        # åˆ—å‡ºæ‰€æœ‰åå¥½
        prefs = await nm.kv.list("preferences", USER_ID)
        if not prefs:
            print("  æ²¡æœ‰ä¿å­˜çš„åå¥½ã€‚")
            return
        print("  ç”¨æˆ·åå¥½ï¼š")
        for p in prefs:
            print(f"    {p.key} = {p.value}")
        return

    if len(parts) == 1:
        # æŸ¥è¯¢å•ä¸ªåå¥½
        val = await nm.kv.get("preferences", USER_ID, parts[0])
        if val is None:
            print(f"  åå¥½ '{parts[0]}' æœªè®¾ç½®ã€‚")
        else:
            print(f"  {parts[0]} = {val.value}")
    else:
        # è®¾ç½®åå¥½
        await nm.kv.set("preferences", USER_ID, parts[0], parts[1])
        print(f"  å·²è®¾ç½®: {parts[0]} = {parts[1]}")


# --- ä¸»å¾ªç¯ ---

HELP_TEXT = """
å¯ç”¨å‘½ä»¤ï¼š
  /memories <query>  - æœç´¢è®°å¿†ï¼ˆä¸‰å› å­è¯„åˆ†ï¼šç›¸å…³æ€§ Ã— æ—¶æ•ˆæ€§ Ã— é‡è¦æ€§ï¼‰
  /reflect           - è§¦å‘åæ€ï¼Œä»è¿‘æœŸè®°å¿†ç”Ÿæˆé«˜å±‚æ¬¡æ´å¯Ÿ
  /history           - æŸ¥çœ‹å½“å‰ä¼šè¯å¯¹è¯å†å²
  /prefs [key] [val] - æŸ¥çœ‹/è®¾ç½®åå¥½
  /help              - æ˜¾ç¤ºå¸®åŠ©
  /quit              - é€€å‡º
"""


async def main():
    database_url = os.getenv(
        "DATABASE_URL",
        "postgresql+asyncpg://neuromemory:neuromemory@localhost:5432/neuromemory",
    )

    # ä½¿ç”¨é»˜è®¤æå–ç­–ç•¥ï¼šæ¯ 10 æ¡æ¶ˆæ¯ + 10 åˆ†é’Ÿç©ºé—² + session å…³é—­ + ç¨‹åºé€€å‡º
    strategy = ExtractionStrategy()

    embedding = create_embedding_provider()
    llm = create_llm_provider()

    # å¼€å¯ extraction æ—¥å¿—ï¼Œä¾¿äºè§‚å¯Ÿè‡ªåŠ¨æå–è¿‡ç¨‹
    logging.basicConfig(level=logging.INFO, format="%(message)s")
    logging.getLogger("neuromemory").setLevel(logging.INFO)

    async with NeuroMemory(
        database_url=database_url,
        embedding=embedding,
        llm=llm,
        extraction=strategy,
        graph_enabled=True,
    ) as nm:
        print("=" * 50)
        print("  NeuroMemory Chat Agent")
        print(f"  ä¼šè¯ ID: {SESSION_ID[:8]}...")
        print("  è¾“å…¥ /help æŸ¥çœ‹å¯ç”¨å‘½ä»¤")
        print("=" * 50)

        history: list[dict] = []

        while True:
            try:
                user_input = input("\nä½ : ").strip()
            except (EOFError, KeyboardInterrupt):
                user_input = "/quit"

            if not user_input:
                continue

            # å¤„ç†å‘½ä»¤
            if user_input.startswith("/"):
                cmd, _, args = user_input[1:].partition(" ")
                cmd = cmd.lower()

                if cmd == "quit" or cmd == "exit":
                    # å…³é—­ sessionï¼Œè§¦å‘è®°å¿†æå–
                    await nm.conversations.close_session(USER_ID, SESSION_ID)
                    print("\nå†è§ï¼")
                    break
                elif cmd == "memories":
                    await handle_memories_command(nm, args.strip())
                elif cmd == "reflect":
                    await handle_reflect_command(nm)
                elif cmd == "history":
                    await handle_history_command(nm)
                elif cmd == "prefs":
                    await handle_prefs_command(nm, args)
                elif cmd == "help":
                    print(HELP_TEXT)
                else:
                    print(f"  æœªçŸ¥å‘½ä»¤: /{cmd}ï¼Œè¾“å…¥ /help æŸ¥çœ‹å¸®åŠ©ã€‚")
                continue

            # 1. æ··åˆæ£€ç´¢ç›¸å…³è®°å¿†
            recall_result = await nm.recall(user_id=USER_ID, query=user_input, limit=3)
            memories = recall_result["merged"]

            # 2. è°ƒç”¨ LLM ç”Ÿæˆå›å¤
            reply = await generate_reply(llm, user_input, memories, history)

            # 3. ä¿å­˜å¯¹è¯æ¶ˆæ¯ï¼ˆæ¡†æ¶è‡ªåŠ¨æŒ‰ç­–ç•¥è§¦å‘è®°å¿†æå–ï¼‰
            await nm.conversations.add_message(
                user_id=USER_ID, role="user", content=user_input, session_id=SESSION_ID,
            )
            await nm.conversations.add_message(
                user_id=USER_ID, role="assistant", content=reply, session_id=SESSION_ID,
            )

            # 4. å°†ç”¨æˆ·æ¶ˆæ¯å­˜ä¸ºè®°å¿†ï¼ˆä¾¿äºåç»­è¯­ä¹‰æœç´¢ï¼‰
            await nm.add_memory(user_id=USER_ID, content=user_input)

            # 5. æ›´æ–°æœ¬åœ°å†å²
            history.append({"role": "user", "content": user_input})
            history.append({"role": "assistant", "content": reply})

            # 6. æ‰“å°å›å¤
            if memories:
                print(f"  (å¬å› {len(memories)} æ¡ç›¸å…³è®°å¿†)")
            print(f"\nåŠ©æ‰‹: {reply}")


if __name__ == "__main__":
    asyncio.run(main())
